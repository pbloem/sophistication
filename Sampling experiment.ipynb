{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Conv2D, Input, UpSampling2D\n",
    "\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Countable Model Class\n",
    "\n",
    "We want a parameterized class of generative networks as models. The parameter values should such that, via a prefix-free encoding, they induce a _meaningful_ prior on the models. We desire a few properties:\n",
    "- Every image of $N$ bits (uncompressed) can be generated by a model taking no input and having a prior probability as close to $2^{-N}$ as possible;\n",
    "- There is a model taking $N$ bits of input such that every image of $N$ bits can be generated by it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we may fix the size of the output image and pretend the size, $N$, is known to a model. To be completely thorough, this would add $\\log N$ bits to the input of the model. The input of the model is treated as an enumeration of the images generated by it, so strictly speaking the model class should come with a way to scale models to arbitrary input sizes.\n",
    "\n",
    "A large-ish image size limits the impact of small constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the generated image\n",
    "height = width = 256\n",
    "channels = 1\n",
    "## With color images (channels = 3), the lack of coherence between channels makes for very noisy images\n",
    "\n",
    "# Let's go with 4 bits per pixel per channel (4096 colors)\n",
    "uncompressed = height * width * channels * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide the uncompressed bits uniformly at random into three types of information\n",
    "_splits = sorted(np.random.choice(uncompressed - 1, 2, replace=False))\n",
    "soph = _splits[0] + 1\n",
    "noise = _splits[1] - _splits[0]\n",
    "redundancy = uncompressed - 1 - _splits[1]\n",
    "\n",
    "print('uncompressed  ', uncompressed)\n",
    "print('sophistication', soph)\n",
    "print('noise         ', noise)\n",
    "print('redundancy    ', redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now tasked with using `soph` bits to initiate a network that takes `noise` bits as input and generates `uncompressed` bits (shaped correctly) as output.\n",
    "When choosing the `soph` bits and the `noise` bits randomly, then, with high probability, the generated image has a _sophistication_ and _noise_ close to `soph` and `noise`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical convolutional neural network (CNN) architecture is `INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC` (http://cs231n.github.io/convolutional-networks/).\n",
    "In order to generate images from unstructured data, we simply reverse this architecture.\n",
    "Note that pooling layers are not invertible.\n",
    "It has been argued that the pooling layers should be replaced by increasing the stride of the convolutions (https://arxiv.org/abs/1412.6806), which suggests a fractional stride for the reverse direction.\n",
    "A fractional stride is implemented by padding each pixel with zeros (indicative of the loss of information in pooling).\n",
    "However, a fractional stride executed like this can introduce artifacts (https://distill.pub/2016/deconv-checkerboard/).\n",
    "Therefore, we choose to invert the pooling layers simply by upscaling.\n",
    "Although ReLU activation is not invertible per se, no expressive power is lost by using ReLU activation for the reverse direction (but filter weights may need to be adjusted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture comes with many hyperparameters that should ideally be parameters of the model class and hence part of the model encoding.\n",
    "For simplicity, we shall fix them outside the model class and note that the total information represented by the parameters may be low in comparison to the sophistication.\n",
    "The hyperparameters are\n",
    "\n",
    "- The number of layers in the fully connected part and for each layer\n",
    "  - The number of cells;\n",
    "- The number of convolution blocks and for each block\n",
    "  - The number of convolutions and for each convolution\n",
    "    - The size of the filter (height, width, depth);\n",
    "\n",
    "Upsampling happens after each block of convolutions and is conveniently done by a factor 2 in the height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters determine the number of weights that must be encoded in the sophistication.\n",
    "Typically, weights in a CNN are not distributed uniformly at random and a trained network can be compressed substantially.\n",
    "We distinguish two approaches to compressing neural networks:\n",
    "1. Invasive (e.g. pruning, [LeCun et al., 1989; Guo et al., 2016])\n",
    "2. Non-invasive (e.g. quantization [Gupta et al., 2015])\n",
    "The second approach comes in fixed-length and variable-length versions and differs in the amount of reproduction accuracy required accross the literature.\n",
    "While variable-length and invasive techniques may produce better results, we choose to focus on fixed-length quantization.\n",
    "If needed, it is possible to account for an additional `X%` improvement obtainable by different methods via an implicit `X/(1-X)%` increase in the available sophistication.\n",
    "\n",
    "Experiments have shown that weights in a CNN typically follow a Laplace (two-sided exponential) distribution that has lower variance in layers further away from the image.\n",
    "Quantizing with such a distribution, around 5 bits of accuracy are needed for acceptable performance (http://vijaychan.github.io/Publications/dcc-2017-cnn-compression.pdf).\n",
    "However, in typical networks (e.g. AlexNet and VGG) the number of weights is dominated by the weights in the fully connected layer, for which our simple quantization scheme is of no use.\n",
    "In deep convolutional networks (e.g. ResNet), most weights reside in convolutional layers.\n",
    "We strive to keep the fully connected layer as small as possible, while still realizing the properties we desire of our model class.\n",
    "In particular, when all information resides in the sophistication, we should resort to a simple fully connected layer where the weight encodes all information in a channel of a pixel (i.e. 4 bits).\n",
    "\n",
    "It has been observed that CNNs with random filters are able to perform well.\n",
    "This suggests that information can be pushed to the fully connected layer.\n",
    "We shall not be too concerned with this and try to put the information in the convolutional layers.\n",
    "\n",
    "From Wikipedia:\n",
    "> **Number of filters**\n",
    ">\n",
    "> Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more.\n",
    "> To equalize computation at each layer, the feature x pixel position product is kept roughly constant across layers.\n",
    "> Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick and Dirty\n",
    "\n",
    "Let's ignore the fully connected layer for a moment.\n",
    "There are roughly `soph // 5` weight variables available (drawn from Laplace distributions with varying variance), assuming 5 bits per weight.\n",
    "\n",
    "The size of the output image is `height * width * channels`, whereas, ideally, the input is `1 * 1 * noise`.\n",
    "Suppose we keep the kernel size fixed throughout the network (**`kernelsize`**) and let the depth of each convolution layer increases exponentially from the image towards the input (**`depthfactor`**), thus decreases with a factor `1 / depthfactor` towards the image.\n",
    "\n",
    "This allows us to calculate the number of weights in the `i`th convolution layer from the image:\n",
    "\n",
    "```kernelsize ** 2 * channels * depthfactor ** (i - 1) * channels * depthfactor ** i```\n",
    "\n",
    "If their are `depth` convolution layers in the network, this amounts to a total number of weights of\n",
    "\n",
    "```kernelsize ** 2 * channels ** 2 * depthfactor / (depthfactor ** 2 - 1) * [depthfactor ** (2 * depth) - 1]```\n",
    "\n",
    "For convenience we match the depth of the input to `channels * depthfactor ** depth`, meaning that it may end up not being sized `1 * 1 * noise`.\n",
    "Importantly, we must have `height / 2 ** depth <= sqrt(noise / (channels * depthfactor ** depth))`.\n",
    "Additionally, we should demand `height >= sqrt(noise / (channels * depthfactor ** depth))` to hold.\n",
    "This assumes we upsample at most every convolution layer and that we upsample with a factor of 2 in both height and width.\n",
    "\n",
    "### Assumptions\n",
    "- we keep the kernelsize constant throughout the network\n",
    "- we distribute the upsampling steps evenly throughout the network\n",
    "- the filterdepth decreases by a constant factor in every block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_weights(kernelsize, depth, depthfactor):\n",
    "    \"\"\"Total number of weights in a network with the given hyperparameters\"\"\"\n",
    "    try:\n",
    "        return kernelsize**2 * channels**2 * depthfactor * (depthfactor**(2 * depth) - 1) / (depthfactor**2 - 1)\n",
    "    except OverflowError:\n",
    "        return float(\"Infinity\")\n",
    "\n",
    "\n",
    "def depth_to_factor(weights, kernelsize, depth):\n",
    "    \"\"\"Find a depthfactor corresponding to a depth given the number of weights\"\"\"\n",
    "    # Test the degenerate limit case where depthfactor = 1\n",
    "    if kernelsize**2 * channels**2 * depth > weights: return None\n",
    "    PRECISION = 30\n",
    "    low, high = (0.0, 1.0)\n",
    "    for _ in range(PRECISION):\n",
    "        candidate = (low + high) / 2\n",
    "        candidate_weights = number_of_weights(kernelsize, depth, 1 / candidate)\n",
    "        if candidate_weights < weights: high = candidate\n",
    "        else: low = candidate\n",
    "\n",
    "    # Last known depthfactor guaranteed to not generate too many weights\n",
    "    return 1 / high\n",
    "\n",
    "\n",
    "def possible_depths(noise, weights, kernelsize):\n",
    "    \"\"\"Generate all possible depths for a given number of weights and kernelsize\"\"\"\n",
    "    depth = 1\n",
    "    while True:\n",
    "        depthfactor = depth_to_factor(weights, kernelsize, depth)\n",
    "        if depthfactor is None: break\n",
    "        reshapesize = np.sqrt(noise / (channels * depthfactor**depth))\n",
    "\n",
    "        if height >= reshapesize and height / 2**depth <= reshapesize:\n",
    "            yield (depth, depthfactor, reshapesize)\n",
    "        depth += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We pick some values for the model parameters ad-hoc and do not treat the length of their specification as part of the sophistication.\n",
    "As the number of parameters is constant, this should not have a large effect on the specifics of the generated imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adhoc_params(soph, noise, verbose=False):\n",
    "    \"\"\"We choose a low networkdepth so that the reshapesize and the spatial randomness are low\"\"\"\n",
    "    params = SimpleNamespace(bpw = 5, kernelsize=7, depth=0, numblocks=0)\n",
    "    for _depth, _depthfactor, _reshapesize in possible_depths(noise, soph / params.bpw, params.kernelsize):\n",
    "        _numblocks = 1 + np.log2(height / _reshapesize)\n",
    "        if verbose:\n",
    "            print(\"Possible depth: {}\\t(number of blocks: {:.3f})\".format(_depth, _numblocks))\n",
    "        if _numblocks >= params.numblocks:\n",
    "            params.numblocks = int(_numblocks)\n",
    "            params.depth = _depth\n",
    "        elif not verbose: break\n",
    "    if params.numblocks == 0: return None\n",
    "\n",
    "    params.blockdepth = int(np.ceil(params.depth / params.numblocks))\n",
    "    params.reshapesize = height // 2**(params.numblocks - 1)\n",
    "    params.reshapedepth = max(int(noise / params.reshapesize**2), channels)\n",
    "    params.depthfactor = (params.reshapedepth / channels)**(1 / params.depth)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "params = adhoc_params(soph, noise, True)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer_qlaplace(bpv, scale, verbose=False):\n",
    "    \"\"\"Construct a generator for elements from a quantized laplace distribution\"\"\"\n",
    "    # ADHOC: We construct 2**bpv + 1 values, so that 0 is included\n",
    "    quantiles = [scale * np.log(2 * (i + 1) / (2**bpv + 2)) for i in range(2**(bpv - 1))]\n",
    "    if verbose:\n",
    "        print(\"effective scaling (bpv={}, scale={}): {}\".format(bpv, scale, np.prod(np.negative(quantiles))))\n",
    "    quantiles = quantiles + [0.0] + [-q for q in quantiles]\n",
    "\n",
    "    def sample(shape, dtype=None):\n",
    "        return np.random.choice(quantiles, size=shape)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def random_model(params, verbose=False):\n",
    "    \"\"\"Sample a model with the given hyperparameters\"\"\"\n",
    "    kernel = (params.kernelsize, params.kernelsize)\n",
    "    x = encoded = Input(shape=(params.reshapesize, params.reshapesize, params.reshapedepth))\n",
    "\n",
    "    for block in range(params.numblocks - 1): # One too few\n",
    "        # ADHOC: We let the variance decay as a function of the block index\n",
    "        initializer = initializer_qlaplace(params.bpw,\n",
    "                                           (1 + (block + 1) / params.numblocks) / params.kernelsize**2,\n",
    "                                           verbose)\n",
    "        for i in range(params.blockdepth):\n",
    "            x = Conv2D(round(params.reshapedepth / params.depthfactor**(block * params.blockdepth + i + 1)),\n",
    "                       kernel, activation='relu', padding='same', use_bias=False,\n",
    "                       kernel_initializer=initializer)(x)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "    # The remaining block (without upsampling)\n",
    "    initializer = initializer_qlaplace(params.bpw, 2 / params.kernelsize**2, verbose)\n",
    "    for i in range((params.numblocks - 1) * params.blockdepth, params.depth):\n",
    "        x = Conv2D(round(params.reshapedepth / params.depthfactor**(i + 1)),\n",
    "                   kernel, activation='relu', padding='same', use_bias=False,\n",
    "                   kernel_initializer=initializer)(x)\n",
    "\n",
    "    # Bring the outputs inside [0, 1] (TODO: 4 bits per channel discretization)\n",
    "    decoded = Activation('sigmoid')(x)\n",
    "\n",
    "    return Model(encoded, decoded)\n",
    "\n",
    "\n",
    "if params is not None:\n",
    "    m = random_model(params, True)\n",
    "    m.summary()\n",
    "    print(\"The number of parameters should be reasonably close to the number\\n\"\n",
    "          \"of weights ({:,.1f}) available from the sophistication.\".format(soph / params.bpw))\n",
    "else:\n",
    "    print(\"No parameters associated with soph={} and noise={}.\".format(soph, noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "\n",
    "try:\n",
    "    z = np.random.choice(2, (n, params.reshapesize, params.reshapesize, params.reshapedepth))\n",
    "    y = m.predict(z, batch_size=n)\n",
    "    if channels == 1: y = y.reshape([n, height, width])\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(2, 3, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(y[i, ...])\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"No model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some variation in parameters!\n",
    "\n",
    "### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image(model):\n",
    "    \"\"\"Make some noise and generate a single image\"\"\"\n",
    "    noise = np.random.choice(2, (1,) + K.int_shape(model.inputs[0])[1:])\n",
    "    image = model.predict(noise, batch_size=1)[0, ...]\n",
    "    if channels == 1: image = image.reshape([height, width])\n",
    "    return image\n",
    "\n",
    "\n",
    "@interact(compression=(0.0, 1.0), sophistication=(0.0, 1.0))\n",
    "def single_show(compression=(soph+noise)/uncompressed, sophistication=soph/(soph+noise)):\n",
    "    soph = int(sophistication * compression * uncompressed)\n",
    "    noise = int(compression * uncompressed - soph + 0.5)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    try:\n",
    "        plt.imshow(\n",
    "            single_image(\n",
    "                random_model(\n",
    "                    adhoc_params(soph, noise)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-interactive (batch mode)\n",
    "\n",
    "Noise increases rightward on the horizontal axis, while sophistication increases downward on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_show(n, save='random-images.pdf'):\n",
    "    step = (uncompressed - 2) // n\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    done = 0\n",
    "    for soph in range(1, n):\n",
    "        for noise in range(1, n - soph + 1):\n",
    "            print(\"\\r{} % \".format(int(100 * done / (n * (n - 1) / 2))), end='')\n",
    "            ax = fig.add_subplot(n - 1, n - 1, (soph - 1) * (n - 1) + noise, xticks=[], yticks=[])\n",
    "            try:\n",
    "                ax.imshow(\n",
    "                    single_image(\n",
    "                        random_model(\n",
    "                            adhoc_params(soph * step, noise * step)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            done += 1\n",
    "\n",
    "    print(\"\\rDone\")\n",
    "    if save is not None: plt.savefig(save)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "multi_show(16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
