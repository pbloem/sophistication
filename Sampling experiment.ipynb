{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Questions/comments are prefixed by ''&lt;author&gt;:''.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random #Jouke: Do we use the former? Can we not just use numpy for the latter?\n",
    "\n",
    "from ipywidgets import interact\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten, Input\n",
    "from keras.layers import Reshape, UpSampling2D\n",
    "import numpy as np\n",
    "\n",
    "import powerlaw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pwl = powerlaw.Power_Law(xmin=1, discrete=True, parameters=[1.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py(d = 0.1):\n",
    "    \"\"\"Something akin to a Pitman-Yor process\"\"\"\n",
    "    members = []\n",
    "    \n",
    "    while True:\n",
    "        if len(members) < 1 or random.random() < d:\n",
    "            draw = random.normalvariate(0, 1.0)\n",
    "        else:\n",
    "            draw = random.choice(members)\n",
    "            \n",
    "        yield draw\n",
    "        members.append(draw)\n",
    "\n",
    "gen = py(0.01)\n",
    "\n",
    "def py_init(shape, dtype=None):\n",
    "    \"\"\"Custom weight initializer for Keras\"\"\" \n",
    "    arr = np.fromfunction(lambda *args: args[0]*0.0 + next(gen), shape=shape)\n",
    "    return arr\n",
    "#     return K.random_normal(shape, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Countable Model Class\n",
    "\n",
    "We want a parameterized class of generative networks as models. The parameter values should such that, via a prefix-free encoding, they induce a _meaningful_ prior on the models. We desire a few properties:\n",
    "- Every image of $N$ bits (uncompressed) can be generated by a model taking no input and having a prior probability as close to $2^{-N}$ as possible;\n",
    "- There is a model taking $N$ bits of input such that every image of $N$ bits can be generated by it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we may fix the size of the output image and pretend the size, $N$, is known to a model. To be completely thorough, this would add $\\log N$ bits to the input of the model. The input of the model is treated as an enumeration of the images generated by it, so strictly speaking the model class should come with a way to scale models to arbitrary input sizes.\n",
    "\n",
    "A large-ish image size limits the impact of small constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the generated image\n",
    "height = width = 128\n",
    "channels = 3\n",
    "\n",
    "# Let's go with 4 bits per pixel per channel (4096 colors)\n",
    "uncompressed = height * width * channels * 4\n",
    "\n",
    "# We divide the uncompressed bits uniformly at random into three types of information\n",
    "_splits = sorted(np.random.choice(uncompressed - 1, 2, replace=False))\n",
    "soph = _splits[0] + 1\n",
    "noise = _splits[1] - _splits[0]\n",
    "redundancy = uncompressed - 1 - _splits[1]\n",
    "\n",
    "@interact(_soph=(1, uncompressed - 2), _noise=(1, uncompressed - 2))\n",
    "def report(_soph=soph, _noise=noise):\n",
    "    global soph, noise, redundancy\n",
    "    if _soph + _noise > uncompressed - 1: _noise = uncompressed - _soph - 1\n",
    "    soph = _soph\n",
    "    noise = _noise\n",
    "    redundancy = uncompressed - _soph - _noise\n",
    "    print('uncompressed  ', uncompressed)\n",
    "    print('sophistication', soph)\n",
    "    print('noise         ', noise)\n",
    "    print('redundancy    ', redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now tasked with using `soph` bits to initiate a network that takes `noise` bits as input and generates `uncompressed` bits (shaped correctly) as output.\n",
    "When choosing the `soph` bits and the `noise` bits randomly, then, with high probability, the generated image has a _sophistication_ and _noise_ close to `soph` and `noise`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical convolutional neural network (CNN) architecture is `INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC` (http://cs231n.github.io/convolutional-networks/).\n",
    "In order to generate images from unstructured data, we simply reverse this architecture.\n",
    "Note that pooling layers are not invertible.\n",
    "It has been argued that the pooling layers should be replaced by increasing the stride of the convolutions (https://arxiv.org/abs/1412.6806), which suggests a fractional stride for the reverse direction.\n",
    "A fractional stride is implemented by padding each pixel with zeros (indicative of the loss of information in pooling).\n",
    "However, a fractional stride executed like this can introduce artifacts (https://distill.pub/2016/deconv-checkerboard/).\n",
    "Therefore, we choose to invert the pooling layers simply by upscaling.\n",
    "Although ReLU activation is not invertible per se, no expressive power is lost by using ReLU activation for the reverse direction (but filter weights may need to be adjusted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture comes with many hyperparameters that should ideally be parameters of the model class and hence part of the model encoding.\n",
    "For simplicity, we shall fix them outside the model class and note that the total information represented by the parameters may be low in comparison to the sophistication.\n",
    "The hyperparameters are\n",
    "\n",
    "- The number of layers in the fully connected part and for each layer\n",
    "  - The number of cells;\n",
    "- The number of convolution blocks and for each block\n",
    "  - The number of convolutions and for each convolution\n",
    "    - The size of the filter (height, width, depth);\n",
    "\n",
    "Upsampling happens after each block of convolutions and is conveniently done by a factor 2 in the height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters determine the number of weights that must be encoded in the sophistication.\n",
    "Typically, weights in a CNN are not distributed uniformly at random and a trained network can be compressed substantially.\n",
    "We distinguish two approaches to compressing neural networks:\n",
    "1. Invasive (e.g. pruning, [LeCun et al., 1989; Guo et al., 2016])\n",
    "2. Non-invasive (e.g. quantization [Gupta et al., 2015])\n",
    "The second approach comes in fixed-length and variable-length versions and differs in the amount of reproduction accuracy required accross the literature.\n",
    "While variable-length and invasive techniques may produce better results, we choose to focus on fixed-length quantization.\n",
    "If needed, it is possible to account for an additional `X%` improvement obtainable by different methods via an implicit `X/(1-X)%` increase in the available sophistication.\n",
    "\n",
    "Experiments have shown that weights in a CNN typically follow a Laplace (two-sided exponential) distribution that has lower variance in layers further away from the image.\n",
    "Quantizing with such a distribution, around 5 bits of accuracy are needed for acceptable performance (http://vijaychan.github.io/Publications/dcc-2017-cnn-compression.pdf).\n",
    "However, in typical networks (e.g. AlexNet and VGG) the number of weights is dominated by the weights in the fully connected layer, for which our simple quantization scheme is of no use.\n",
    "In deep convolutional networks (e.g. ResNet), most weights reside in convolutional layers.\n",
    "We strive to keep the fully connected layer as small as possible, while still realizing the properties we desire of our model class.\n",
    "In particular, when all information resides in the sophistication, we should resort to a simple fully connected layer where the weight encodes all information in a channel of a pixel (i.e. 4 bits).\n",
    "\n",
    "It has been observed that CNNs with random filters are able to perform well.\n",
    "This suggests that information can be pushed to the fully connected layer.\n",
    "We shall not be too concerned with this and try to put the information in the convolutional layers.\n",
    "\n",
    "From Wikipedia:\n",
    "> **Number of filters**\n",
    ">\n",
    "> Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more.\n",
    "> To equalize computation at each layer, the feature x pixel position product is kept roughly constant across layers.\n",
    "> Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick and Dirty\n",
    "\n",
    "Let's ignore the fully connected layer for a moment.\n",
    "There are roughly `soph // 5` weight variables available (drawn from Laplace distributions with varying variance), assuming 5 bits per weight.\n",
    "\n",
    "The size of the output image is `height * width * channels`, whereas, ideally, the input is `1 * 1 * noise`.\n",
    "Suppose we keep the kernel size fixed throughout the network (**`kernelsize`**) and let the depth of each convolution layer increases exponentially from the image towards the input (**`depthfactor`**), thus decreases with a factor `1 / depthfactor` towards the image.\n",
    "\n",
    "This allows us to calculate the number of weights in the `i`th convolution layer from the image:\n",
    "\n",
    "```kernelsize ** 2 * channels * depthfactor ** (i - 1) * channels * depthfactor ** i```\n",
    "\n",
    "If their are `depth` convolution layers in the network, this amounts to a total number of weights of\n",
    "\n",
    "```kernelsize ** 2 * channels ** 2 * depthfactor / (depthfactor ** 2 - 1) * [depthfactor ** (2 * depth) - 1]```\n",
    "\n",
    "For convenience we match the depth of the input to `channels * depthfactor ** depth`, meaning that it may end up not being sized `1 * 1 * noise`.\n",
    "Importantly, we must have `height / 2 ** depth <= sqrt(noise / (channels * depthfactor ** depth))`.\n",
    "Additionally, we should demand `height >= sqrt(noise / (channels * depthfactor ** depth))` to hold.\n",
    "This assumes we upsample at most every convolution layer and that we upsample with a factor of 2 in both height and width.\n",
    "\n",
    "### Assumptions\n",
    "- we keep the kernelsize constant throughout the network\n",
    "- we distribute the upsampling steps evenly throughout the network\n",
    "- the filterdepth decreases by a constant factor in every block\n",
    "- we treat the noise as a bitstring instead of as a string of, say, 4 bit words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_weights(kernelsize, depth, depthfactor):\n",
    "    \"\"\"total number of weights in a network with the given hyperparameters\"\"\"\n",
    "    return kernelsize**2 * channels**2 * depthfactor * (depthfactor**(2 * depth) - 1) / (depthfactor**2 - 1)\n",
    "\n",
    "def depth_to_factor(weights, kernelsize, depth):\n",
    "    \"\"\"find a depthfactor corresponding to a depth given the number of weights\"\"\"\n",
    "    # Test the degenerate limit case where depthfactor = 1\n",
    "    if kernelsize**2 * channels**2 * depth > weights: return None\n",
    "    PRECISION = 30\n",
    "    low, high = (0.0, 1.0)\n",
    "    for _ in range(PRECISION):\n",
    "        candidate = (low + high) / 2\n",
    "        candidate_weights = number_of_weights(kernelsize, depth, 1 / candidate)\n",
    "        if candidate_weights < weights: high = candidate\n",
    "        else: low = candidate\n",
    "    # Last known depthfactor guaranteed to not generate too many weights\n",
    "    return 1 / high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_depths():\n",
    "    depth = 1\n",
    "    while True:\n",
    "        depthfactor = depth_to_factor(weights, kernelsize, depth)\n",
    "        if depthfactor is None: break\n",
    "        reshapesize = np.sqrt(noise / (channels * depthfactor**depth))\n",
    "\n",
    "        if height >= reshapesize and height / 2**depth <= reshapesize:\n",
    "            yield (depth, depthfactor, reshapesize)\n",
    "        depth += 1\n",
    "\n",
    "\n",
    "# Out-of-band parameters\n",
    "kernelsize = 5\n",
    "bpw = 5\n",
    "\n",
    "# Derived parameters\n",
    "weights = soph / bpw\n",
    "\n",
    "# ADHOC: We choose a low network depth so that the reshapesize and the spatial randomness are low\n",
    "numblocks = None\n",
    "for _depth, _depthfactor, _reshapesize in possible_depths():\n",
    "    _numblocks = 1 + np.log2(height / _reshapesize)\n",
    "    print(\"Possible depth: {}\\t(number of blocks: {:.3f})\".format(_depth, _numblocks))\n",
    "    if numblocks is None or _numblocks >= numblocks:\n",
    "        numblocks = int(_numblocks)\n",
    "        depth = _depth\n",
    "\n",
    "blockdepth = int(np.ceil(depth / numblocks))\n",
    "reshapesize = height // 2**(numblocks - 1)\n",
    "reshapedepth = max(int(noise / reshapesize**2), channels)\n",
    "depthfactor = (reshapedepth / channels)**(1 / depth)\n",
    "print('weights     ', weights)\n",
    "print('kernelsize  ', kernelsize)\n",
    "print('depth       ', depth)\n",
    "print('numblocks   ', numblocks)\n",
    "print('blockdepth  ', blockdepth)\n",
    "print('reshapesize ', reshapesize)\n",
    "print('reshapedepth', reshapedepth)\n",
    "print('depthfactor ', depthfactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer_qlaplace(bps, scale):\n",
    "    \"\"\"construct a generator for elements from a quantized laplace distribution\"\"\"\n",
    "    # ADHOC: We construct 2**bps + 1 values, so that 0 is included\n",
    "    quantiles = [scale * np.log(2 * (i + 1) / (2**bps + 2)) for i in range(2**(bps - 1))]\n",
    "    print(\"effective scaling (bps={}, scale={}): {}\".format(bps, scale, np.prod(np.negative(quantiles))))\n",
    "    quantiles = quantiles + [0.0] + [-q for q in quantiles]\n",
    "    def sample(shape, dtype=None):\n",
    "        return np.random.choice(quantiles, size=shape)\n",
    "    return sample\n",
    "\n",
    "\n",
    "x = encoded = Input(shape=(reshapesize, reshapesize, reshapedepth))\n",
    "for block in range(numblocks - 1): # One too few\n",
    "    # ADHOC: We let the variance decay as a function of the block index\n",
    "    initializer = initializer_qlaplace(bpw, 2 * (block + 1) / numblocks)\n",
    "    for i in range(blockdepth):\n",
    "        x = Conv2D(int(np.ceil(reshapedepth / depthfactor ** (block * blockdepth + i + 1))),\n",
    "                   (kernelsize, kernelsize),\n",
    "                   activation='relu', padding='same', use_bias=False,\n",
    "                   kernel_initializer=initializer)(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "for i in range(depth - (numblocks - 1) * blockdepth): # The remaining one (without upsampling)\n",
    "    x = Conv2D(int(np.ceil(reshapedepth / depthfactor ** ((numblocks - 1) * blockdepth + i + 1))),\n",
    "               (kernelsize, kernelsize),\n",
    "               activation='relu', padding='same', use_bias=False,\n",
    "               kernel_initializer=initializer_qlaplace(bpw, 2))(x)\n",
    "\n",
    "# Bring the outputs inside [0, 1] (TODO: 4 bits per channel discretization)\n",
    "decoded = Activation('sigmoid')(x)\n",
    "\n",
    "decoder = Model(encoded, decoded)\n",
    "\n",
    "decoder.summary()\n",
    "print(\"The number of parameters should be reasonably close to the number\\n\"\n",
    "      \"of weights ({:,.1f}) available from the sophistication.\".format(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "\n",
    "z = np.random.choice(2, (n, reshapesize, reshapesize, reshapedepth))\n",
    "y = decoder.predict(z, batch_size=n)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(2, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(y[i, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peter's Model\n",
    "\n",
    "Currently, the topology is a series of `numblocks` stacks of `blockdepth` consecutive 2D convolutions, separated by upsampling layers.\n",
    "There is also a dense layer following the input.\n",
    "This topology is fairly standard (http://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "**Questions Jouke has**:\n",
    "- To what extent is a this standard network layout (`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`, see the above Stanford link) theoretically invertible?\n",
    "- What is the difference between convolutions and transpose convolutions (from https://arxiv.org/pdf/1603.07285.pdf I understand that each can be expressed as the other), and why do we prefer the former?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a model\n",
    "\n",
    "# number of blocks of k convolutions and \n",
    "input_dim = int(pwl.generate_random()[0])\n",
    "print('input_dim\\t', input_dim)\n",
    "\n",
    "num_channels = int(pwl.generate_random()[0])\n",
    "print('num_channels\\t', num_channels)\n",
    "\n",
    "numblocks = random.randint(1,13) # this should be unbounded for a proper model class\n",
    "print('numblocks\\t', numblocks)\n",
    "\n",
    "blockdepth = random.randint(1,7)\n",
    "print('blockdepth\\t', blockdepth)\n",
    "\n",
    "kernelsize = random.randint(1,10)\n",
    "print('kernelsize\\t', kernelsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the PY weight generator\n",
    "# gen=py(0.2)\n",
    "\n",
    "encoded = Input(shape=(input_dim,))\n",
    "\n",
    "x = Dense(256, activation='relu')(encoded)\n",
    "x = Reshape((4,4,16))(x)\n",
    "\n",
    "for n in range(numblocks):\n",
    "    for b in range(blockdepth):\n",
    "        x = Conv2D(num_channels, (kernelsize, kernelsize), activation='relu', padding='same', kernel_initializer=py_init)(x)\n",
    "        \n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "        \n",
    "decoded = Conv2D(3, (4, 4), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "decoder = Model(encoded, decoded)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "\n",
    "z = np.random.randn(n, input_dim) #Jouke: why take a normal distribution over the input?\n",
    "y = decoder.predict(z)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "# plot several images\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(y[i,...])\n",
    "    \n",
    "plt.savefig('random-images.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
